---
title: ""
author: "Allen, Joe, Madi"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Report for project 4 on Linear Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<a id="top"></a>
<h1>Linear Models L1</h1>
### Joe Eppinger, Madison Boman, and Allen Clarke
#### NAU May 3, 2019

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<h3>LMSquareLoss Prostate Dataset Analysis</h3>
<p>This is calculating the L1-Prediction, Baseline on the LinearModelL1CV function we have coded. The data set being used is Prostate</p>
```{r,fig.width = 6}
data <- data(prostate, package="ElemStatLearn")
data.set <- prostate[, -dim(prostate)[2]]
data.set$labels <- data.set[, dim(data.set)[2]]
data.set$features <- data.set[,-dim(data.set)[2]]
x.mat <- as.matrix(prostate[,-1])
max.iterations <- 10
n.folds <- 5
step.size <-.5
final.mat <- matrix(1:10, nrow = 5, dimnames = list(c("Set-1","Set-2","Set-3","Set-4","Set-5"), c("L1-regularized Predictor","Baseline")))
fold.vec <- as.numeric(sample(rep(1:n.folds,l=nrow(data.set$features))))
#starts at 1
for(test.fold in 1:n.folds){
  is.test <- fold.vec == test.fold
  is.train <- !is.test
  x.train <- data.matrix(data.set$features[fold.vec,])
  y.train <- as.vector(data.set$labels[fold.vec])
  penalty.vec = seq(1, 0.1, -0.1)
  x.test <- x.mat[is.test,]
  baseline <- mean(y.train)

  cv.list <- linearmodels:: LinearModelL1CV(x.train,y.train,fold.vec,n.folds,penalty.vec,step.size)
  init.weight.vec <- cv.list$weight.vec
  predict.func <- cv.list$predict(x.train)
  mean.train.loss.vec <- cv.list$mean.train.loss.vec
  mean.validation.loss.vec <- cv.list$mean.validation.loss.vec
  selected.penalty <- cv.list$selected.penalty

  #Inserts into our final matrix
  #L1 regularized predictor
  final.mat[test.fold,1] = mean(predict.func)
  #baseline/un-informed predictor
  final.mat[test.fold,2] = baseline
  cv.list
}
library(gridExtra)
library(grid)
d <- head(final.mat)
grid.table(d)
plot(mean.validation.loss.vec,type="l")
plot(mean.train.loss.vec,type="l")
```
<h4>Analysis of Prostate Data Set</h4>
<p>By observing this dataset we can conclude that the L2 is more accurate than the Early-Stopping. L1 has more accuracy compared to the Baseline since we are approaching 0 more. Whereas Baseline is further away</p>

<h3>LMSquareLoss Ozone Dataset Analysis</h3>
<p>This is calculating the L1-Prediction Baseline on the LinearModelL1CV function we have coded. The data set being used is Ozone</p>
```{r,fig.width = 6}
data <- data(ozone, package="ElemStatLearn")
data.set <- ozone[, -dim(ozone)[2]]
data.set$labels <- data.set[, dim(data.set)[2]]
data.set$features <- data.set[,-dim(data.set)[2]]
x.mat <- as.matrix(ozone[,-1])
max.iterations <- 10
n.folds <- 5
step.size <-.5
final.mat <- matrix(1:10, nrow = 5, dimnames = list(c("Set-1","Set-2","Set-3","Set-4","Set-5"), c("L1-regularized Predictor","Baseline")))
fold.vec <- as.numeric(sample(rep(1:n.folds,l=nrow(data.set$features))))
#starts at 1
for(test.fold in 1:n.folds){
  is.test <- fold.vec == test.fold
  is.train <- !is.test
  x.train <- data.matrix(data.set$features[fold.vec,])
  y.train <- as.vector(data.set$labels[fold.vec])
  penalty.vec = seq(1, 0.1, -0.1)
  x.test <- x.mat[is.test,]
  baseline <- mean(y.train)

  cv.list <- linearmodels:: LinearModelL1CV(x.train,y.train,fold.vec,n.folds,penalty.vec,step.size)
  init.weight.vec <- cv.list$weight.vec
  predict.func <- cv.list$predict(x.train)
  mean.train.loss.vec <- cv.list$mean.train.loss.vec
  mean.validation.loss.vec <- cv.list$mean.validation.loss.vec
  selected.penalty <- cv.list$selected.penalty

  #Inserts into our final matrix
  #L1 regularized predictor
  final.mat[test.fold,1] = mean(predict.func)
  #baseline/un-informed predictor
  final.mat[test.fold,2] = baseline
  cv.list
}
library(gridExtra)
library(grid)
d <- head(final.mat)
grid.table(d)
plot(mean.validation.loss.vec,type="l")
plot(mean.train.loss.vec,type="l")
```
<h4>Analysis of Ozone Data Set</h4>
<p>By observing this dataset we can conclude that the L2 is more accurate than the Early-Stopping. L1 has more accuracy compared to the Baseline since we are approaching 0 more. Whereas Baseline is further away</p>

<h3>LMLogisticLossIterations Spam</h3>
<p>This is calculating the L1-Prediction and Baseline on the LinearModelL1CV function we have coded. The data set being used is Spam</p>
```{r,fig.width = 6}
data(spam, package="ElemStatLearn")
data.set <- spam[, -dim(spam)[2]]
data.set$labels <- data.set[, dim(data.set)[2]]
data.set$features <- data.set[,-dim(data.set)[2]]
set.seed(1)
n.folds <-5
step.size <- .5
fold.vec <- sample(rep(1:n.folds,l=nrow(data.set$features)))
final.mat <- matrix(1:10, nrow = 5, dimnames = list(c("Set-1","Set-2","Set-3","Set-4","Set-5"), c("L1-regularized Predictor","Baseline")))
result.mat.list <- list()
for(test.fold in 1:n.folds){
  is.test <- fold.vec == test.fold
  is.train <- !is.test
  x.train <- data.matrix(data.set$features[fold.vec,])
  y.train <- as.vector(data.set$labels[fold.vec])
  penalty.vec = seq(1, 0.1, -0.1)
  baseline <- mean(y.train)

  cv.list <- linearmodels:: LinearModelL1CV(x.train,y.train,fold.vec,n.folds,penalty.vec,step.size)
  init.weight.vec <- cv.list$weight.vec
  predict.func <- cv.list$predict(x.train)
  mean.train.loss.vec <- cv.list$mean.train.loss.vec
  mean.validation.loss.vec <- cv.list$mean.validation.loss.vec
  selected.penalty <- cv.list$selected.penalty

  #Inserts into our final matrix
  #L1 regularized predictor
  final.mat[test.fold,1] = mean(predict.func)
  #baseline/un-informed predictor
  final.mat[test.fold,2] = baseline
  cv.list
}
library(gridExtra)
library(grid)
d <- head(final.mat)
grid.table(d)
plot(mean.validation.loss.vec,type="l")
plot(mean.train.loss.vec,type="l")
```
<h4>Analysis of Spam Data Set</h4>
<p>By observing this dataset we can conclude that the L2 is more accurate than the Early-Stopping. L1 has more accuracy compared to the Baseline since we are approaching 0 more. Whereas Baseline is further away</p>


<h3>LMLogisticLossIterations SAheart</h3>
<p>This is calculating the L1-Prediction and Baseline on the LinearModelL1CV function we have coded. The data set being used is SAheart</p>
```{r,fig.width = 6}
data(SAheart, package="ElemStatLearn")
data.set <- SAheart[, -dim(SAheart)[2]]
data.set$labels <- data.set[, dim(data.set)[2]]
data.set$features <- data.set[,-dim(data.set)[2]]
set.seed(1)
n.folds <- 5
fold.vec <- sample(rep(1:n.folds,l=nrow(data.set$features)))
final.mat <- matrix(1:10, nrow = 5, dimnames = list(c("Set-1","Set-2","Set-3","Set-4","Set-5"), c("L1-regularized Predictor","Baseline")))
result.mat.list <- list()
for(test.fold in 1:n.folds){
  is.test <- fold.vec == test.fold
  is.train <- !is.test
  x.train <- as.matrix(data.set$features[fold.vec,-5])
  x1.train<- x.train
  y.train <- data.set$labels[fold.vec]
  sub.fold.vec <- sample(rep(1:n.folds,l=nrow(data.set$features)))
  penalty.vec = seq(1, 0.1, -0.1)
  step.size <- .5
  baseline <- mean(y.train)

  cv.list <- linearmodels:: LinearModelL1CV(x.train,y.train,sub.fold.vec,n.folds,penalty.vec,step.size)
  init.weight.vec <- cv.list$weight.vec
  predict.func <- cv.list$predict(x1.train)
  mean.train.loss.vec <- cv.list$mean.train.loss.vec
  mean.validation.loss.vec <- cv.list$mean.validation.loss.vec
  selected.penalty <- cv.list$selected.penalty

  #Inserts into our final matrix
  #L1 regularized predictor
  final.mat[test.fold,1] = mean(predict.func)
  #baseline/un-informed predictor
  final.mat[test.fold,2] = baseline
  cv.list
  }
library(gridExtra)
library(grid)
d <- head(final.mat)
grid.table(d)
plot(mean.validation.loss.vec,type="l")
plot(mean.train.loss.vec,type="l")
```
<h4>Analysis of SAheart Data Set</h4>
<p>By observing this dataset we can conclude that the L2 is more accurate than the Early-Stopping. L1 has more accuracy compared to the Baseline since we are approaching 0 more. Whereas Baseline is further away</p>


<h3>LMLogisticLossIterations zip.train</h3>
<p>This is calculating the L2-Prediction, and Baseline on the LinearModelL1CV function we have coded. The data set being used is zip.train</p>
```{r,fig.width = 6}
data(spam, package="ElemStatLearn")
data(zip.train, package = "ElemStatLearn")
is.01 <- zip.train[,1] %in% c(0,1)
data.list <- list(
  spam=list(
    features=as.matrix(spam[,1:57]),
    labels=ifelse(spam$spam=="spam",1,0)),
  zip.train=list(
    features=zip.train[is.01,-1],
    labels=as.integer(zip.train[is.01,1])))
n.folds <- 4
results.list <- list()
mean.loss.list <- list()
final.mat <- matrix(1:12, nrow = n.folds, dimnames = list(c("Set-1","Set-2","Set-3","Set-4"), c("Early-Stop","L2-Predict","Baseline")))
for(data.name in names(data.list)){
  data.set <- data.list[[data.name]]
  stopifnot(all(data.set$labels %in% c(0,1)))
  stopifnot(length(data.set$labels)==nrow(data.set$features))
  set.seed(1)
  fold.vec <- as.numeric(sample(rep(1:n.folds,l=nrow(data.set$features))))
  result.mat.list <- list()
  for(test.fold in 1:n.folds){
    is.test <- fold.vec == test.fold
    is.train <- !is.test
    x.train <- as.matrix(data.set$features[fold.vec,])
    y.train <- as.vector(data.set$labels[fold.vec])
    sub.fold.vec <- as.vector(sample(rep(1:n.folds,l=nrow(x.train))))
    baseline <- mean(y.train)
    penalty.vec = seq(1, 0.1, -0.1)
    step.size <-.5
  
    cv.list <- linearmodels:: LinearModelL1CV(x.train,y.train,sub.fold.vec,n.folds,penalty.vec,step.size)
    init.weight.vec <- cv.list$weight.vec
    predict.func <- cv.list$predict(x.train)
    mean.train.loss.vec <- cv.list$mean.train.loss.vec
    mean.validation.loss.vec <- cv.list$mean.validation.loss.vec
    selected.penalty <- cv.list$selected.penalty
    
    #Inserts into our final matrix
    #L1 regularized predictor
    final.mat[test.fold,1] = mean(predict.func)
    #baseline/un-informed predictor
    final.mat[test.fold,2] = baseline
    cv.list
  }
}
library(gridExtra)
library(grid)
d <- head(final.mat)
grid.table(d)
plot(mean.validation.loss.vec,type="l")
plot(mean.train.loss.vec,type="l")
```
<h4>Analysis of zip.train Data Set</h4>
<p>By observing this dataset we can conclude that the L2 is more accurate than the Early-Stopping. L1 has more accuracy compared to the Baseline since we are approaching 0 more. Whereas Baseline is further away</p>
